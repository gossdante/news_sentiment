{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage API Key & Use NewsApiClient\n",
    "I've saved my API key in a config file, I've loaded the file and saved my key to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "APIKEY=config.APIKEY\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "newsapi = NewsApiClient(api_key=APIKEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the available sources?\n",
    "I've created a loop to gather all the available sources and save into a dataframe. For clarity sake I've printed the available sources as well as the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many total sources are available?: 128\n",
      "What are the categories?  ['general' 'business' 'technology' 'sports' 'entertainment' 'health'\n",
      " 'science']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sources = newsapi.get_sources()\n",
    "print('How many total sources are available?:',len(sources['sources']))\n",
    "total_sources = len(sources['sources'])\n",
    "sources['sources'][0]\n",
    "\n",
    "id=pd.Series(name='id')\n",
    "name=pd.Series(name='name')\n",
    "description=pd.Series(name='description')\n",
    "url=pd.Series(name='url')\n",
    "category=pd.Series(name='category')\n",
    "language=pd.Series(name='language')\n",
    "country=pd.Series(name='country')\n",
    "#column_names = ['id', 'name', 'description', 'url', 'category', 'language', 'country']\n",
    "\n",
    "for idx,outlet in enumerate(sources['sources']):\n",
    "    id[idx]=outlet['id']\n",
    "    name[idx] = outlet['name']\n",
    "    description[idx] = outlet['description']\n",
    "    url[idx] = outlet['url']\n",
    "    category[idx] = outlet['category']\n",
    "    language[idx] = outlet['language']\n",
    "    country[idx] = outlet['country']\n",
    "sourcedf=pd.concat([id,name,description,url,category,language,country],axis=1)\n",
    "\n",
    "print('What are the categories? ', sourcedf['category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the available sources\n",
    "Requirements: English Language, US Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc-news</td>\n",
       "      <td>ABC News</td>\n",
       "      <td>Your trusted source for breaking news, analysi...</td>\n",
       "      <td>https://abcnews.go.com</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al-jazeera-english</td>\n",
       "      <td>Al Jazeera English</td>\n",
       "      <td>News, analysis from the Middle East and worldw...</td>\n",
       "      <td>http://www.aljazeera.com</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ars-technica</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>The PC enthusiast's resource. Power users and ...</td>\n",
       "      <td>http://arstechnica.com</td>\n",
       "      <td>technology</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>associated-press</td>\n",
       "      <td>Associated Press</td>\n",
       "      <td>The AP delivers in-depth coverage on the inter...</td>\n",
       "      <td>https://apnews.com/</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>axios</td>\n",
       "      <td>Axios</td>\n",
       "      <td>Axios are a new media company delivering vital...</td>\n",
       "      <td>https://www.axios.com</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                name  \\\n",
       "0             abc-news            ABC News   \n",
       "3   al-jazeera-english  Al Jazeera English   \n",
       "6         ars-technica        Ars Technica   \n",
       "8     associated-press    Associated Press   \n",
       "10               axios               Axios   \n",
       "\n",
       "                                          description  \\\n",
       "0   Your trusted source for breaking news, analysi...   \n",
       "3   News, analysis from the Middle East and worldw...   \n",
       "6   The PC enthusiast's resource. Power users and ...   \n",
       "8   The AP delivers in-depth coverage on the inter...   \n",
       "10  Axios are a new media company delivering vital...   \n",
       "\n",
       "                         url    category language country  \n",
       "0     https://abcnews.go.com     general       en      us  \n",
       "3   http://www.aljazeera.com     general       en      us  \n",
       "6     http://arstechnica.com  technology       en      us  \n",
       "8        https://apnews.com/     general       en      us  \n",
       "10     https://www.axios.com     general       en      us  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sourcedf= sourcedf[(sourcedf['language']=='en') & (sourcedf['country']=='us')]\n",
    "sourcedf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the business sources?\n",
    "First we isolate the business sources based on the category variable.\n",
    "I've displayed the number of business sources as a sanity check, as well as tested that the indexing was working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>bloomberg</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>Bloomberg delivers business and markets news, ...</td>\n",
       "      <td>http://www.bloomberg.com</td>\n",
       "      <td>business</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>business-insider</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>Business Insider is a fast-growing business si...</td>\n",
       "      <td>http://www.businessinsider.com</td>\n",
       "      <td>business</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>fortune</td>\n",
       "      <td>Fortune</td>\n",
       "      <td>Fortune 500 Daily and Breaking Business News</td>\n",
       "      <td>http://fortune.com</td>\n",
       "      <td>business</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117</td>\n",
       "      <td>the-wall-street-journal</td>\n",
       "      <td>The Wall Street Journal</td>\n",
       "      <td>WSJ online coverage of breaking news and curre...</td>\n",
       "      <td>http://www.wsj.com</td>\n",
       "      <td>business</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                       id                     name  \\\n",
       "0     16                bloomberg                Bloomberg   \n",
       "1     18         business-insider         Business Insider   \n",
       "2     36                  fortune                  Fortune   \n",
       "3    117  the-wall-street-journal  The Wall Street Journal   \n",
       "\n",
       "                                         description  \\\n",
       "0  Bloomberg delivers business and markets news, ...   \n",
       "1  Business Insider is a fast-growing business si...   \n",
       "2       Fortune 500 Daily and Breaking Business News   \n",
       "3  WSJ online coverage of breaking news and curre...   \n",
       "\n",
       "                              url  category language country  \n",
       "0        http://www.bloomberg.com  business       en      us  \n",
       "1  http://www.businessinsider.com  business       en      us  \n",
       "2              http://fortune.com  business       en      us  \n",
       "3              http://www.wsj.com  business       en      us  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sources:  4\n"
     ]
    }
   ],
   "source": [
    "business_sources = sourcedf[sourcedf['category']=='business']\n",
    "business_sources = business_sources.reset_index()\n",
    "display(business_sources)\n",
    "print('Number of sources: ',len(business_sources))\n",
    "#business_sources['id'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get 30 Days Ago Date in the Proper Format\n",
    "For the loop below to work (on the free tier) we need to only check back 30 days. We will use the datetime package to get todays date and 30 days prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todays Date:  2023-09-11\n",
      "30 Days Ago:  2023-08-12\n"
     ]
    }
   ],
   "source": [
    "# Get 30 Days Ago\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "print('Todays Date: ', date.today().isoformat())\n",
    "print('30 Days Ago: ', (date.today()-timedelta(days=30)).isoformat())\n",
    "startdate=(date.today()-timedelta(days=30)).isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop through all articles \n",
    "There are two layers to this loop, the outer layer loops through the number of business sources (4), while the inner loop gets all the articles from that business source and collects the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business-insider</td>\n",
       "      <td>Lakshmi Varanasi</td>\n",
       "      <td>Apple is reportedly working on the Watch's big...</td>\n",
       "      <td>Apple is planning to release a new model of th...</td>\n",
       "      <td>https://www.businessinsider.com/apple-working-...</td>\n",
       "      <td>2023-08-14T16:03:56Z</td>\n",
       "      <td>Apple has a major redesign of the Apple Watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business-insider</td>\n",
       "      <td>htan@insider.com (Huileng Tan)</td>\n",
       "      <td>The billionaire founder of a key Apple supplie...</td>\n",
       "      <td>He said no foreign investor would dare invest ...</td>\n",
       "      <td>https://www.businessinsider.com/foxconn-terry-...</td>\n",
       "      <td>2023-08-28T10:21:20Z</td>\n",
       "      <td>Terry Gou, the billionaire founder of Foxconn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business-insider</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>Big Tech salaries revealed: This is what devel...</td>\n",
       "      <td>Big tech salaries unveil earnings of engineers...</td>\n",
       "      <td>https://www.businessinsider.com/big-tech-salar...</td>\n",
       "      <td>2023-08-23T16:47:20Z</td>\n",
       "      <td>Business Insider analyzed salary data for work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business-insider</td>\n",
       "      <td>Zahra Tayeb</td>\n",
       "      <td>Apple, Microsoft, Tesla, and Meta see a combin...</td>\n",
       "      <td>Four Big Tech companies - Apple, Microsoft, Te...</td>\n",
       "      <td>https://markets.businessinsider.com/news/stock...</td>\n",
       "      <td>2023-08-26T10:08:01Z</td>\n",
       "      <td>(Photo by Scott Heins/Getty Images)\\r\\n&lt;ul&gt;\\n&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business-insider</td>\n",
       "      <td>Hasan Chowdhury</td>\n",
       "      <td>China is pulling every lever to kill Apple's i...</td>\n",
       "      <td>China is hugely important to Apple. It's doing...</td>\n",
       "      <td>https://www.businessinsider.com/apple-iphone-1...</td>\n",
       "      <td>2023-09-07T11:47:14Z</td>\n",
       "      <td>Apple's Tim Cook must prepare for a possible b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             source                          author  \\\n",
       "0  business-insider                Lakshmi Varanasi   \n",
       "1  business-insider  htan@insider.com (Huileng Tan)   \n",
       "2  business-insider                Business Insider   \n",
       "3  business-insider                     Zahra Tayeb   \n",
       "4  business-insider                 Hasan Chowdhury   \n",
       "\n",
       "                                               title  \\\n",
       "0  Apple is reportedly working on the Watch's big...   \n",
       "1  The billionaire founder of a key Apple supplie...   \n",
       "2  Big Tech salaries revealed: This is what devel...   \n",
       "3  Apple, Microsoft, Tesla, and Meta see a combin...   \n",
       "4  China is pulling every lever to kill Apple's i...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Apple is planning to release a new model of th...   \n",
       "1  He said no foreign investor would dare invest ...   \n",
       "2  Big tech salaries unveil earnings of engineers...   \n",
       "3  Four Big Tech companies - Apple, Microsoft, Te...   \n",
       "4  China is hugely important to Apple. It's doing...   \n",
       "\n",
       "                                                 url           publishedAt  \\\n",
       "0  https://www.businessinsider.com/apple-working-...  2023-08-14T16:03:56Z   \n",
       "1  https://www.businessinsider.com/foxconn-terry-...  2023-08-28T10:21:20Z   \n",
       "2  https://www.businessinsider.com/big-tech-salar...  2023-08-23T16:47:20Z   \n",
       "3  https://markets.businessinsider.com/news/stock...  2023-08-26T10:08:01Z   \n",
       "4  https://www.businessinsider.com/apple-iphone-1...  2023-09-07T11:47:14Z   \n",
       "\n",
       "                                             content  \n",
       "0  Apple has a major redesign of the Apple Watch ...  \n",
       "1  Terry Gou, the billionaire founder of Foxconn ...  \n",
       "2  Business Insider analyzed salary data for work...  \n",
       "3  (Photo by Scott Heins/Getty Images)\\r\\n<ul>\\n<...  \n",
       "4  Apple's Tim Cook must prepare for a possible b...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init Variables\n",
    "Source=pd.Series(name='source')\n",
    "author=pd.Series(name='author')\n",
    "title=pd.Series(name='title')\n",
    "description=pd.Series(name='description')\n",
    "url=pd.Series(name='url')\n",
    "publishedAt=pd.Series(name='publishedAt')\n",
    "content=pd.Series(name='content')\n",
    "# Use Counter\n",
    "counter=0\n",
    "# Loop through each source \n",
    "for source in business_sources['id']:\n",
    "    news_articles = newsapi.get_everything(q='apple',\n",
    "                                          sources=source,\n",
    "                                          from_param=startdate,\n",
    "                                          )\n",
    "    # Gather data from each article\n",
    "    for idx, article in enumerate(news_articles['articles']):\n",
    "        Source[counter] = article['source']['id']\n",
    "        author[counter] = article['author']\n",
    "        title[counter] = article['title']\n",
    "        description[counter] = article['description']\n",
    "        url[counter] = article['url']\n",
    "        publishedAt[counter] = article['publishedAt']\n",
    "        content[counter] = article['content']\n",
    "        #author=pd.concat([Source,article['author']],axis=1)\n",
    "        #for details in article:\n",
    "        #    print(details['title'])\n",
    "        counter = counter + 1\n",
    "# Combine \n",
    "all_articles = pd.concat([Source, author, title, description, url, publishedAt, content],axis=1)\n",
    "all_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "The time has finally come for the namesake of this workbook... Sentiment Analysis.\n",
    "\n",
    "We will attempt a few different methods for sentiment analysis. We'll be using a BERT Transformer model (The \"T\" in BERT), which is a natural language processing model released by google that uses deep learning (convolutional neural networks and recurrent neural networks) to \"understand\" natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ddg12\\OneDrive\\Desktop\\news_sentiment\\sentiment.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   logits \u001b[39m=\u001b[39m model(input_ids)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmax(logits)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(preds)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# get the predicted label\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: max(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# Load BERT Model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# preprocess the text\n",
    "text = \"This is a great movie! I really enjoyed it.\"\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True,max_length=128)])\n",
    "\n",
    "# classify the text\n",
    "with torch.no_grad():\n",
    "  logits = model(input_ids)\n",
    "\n",
    "preds = torch.max(logits)\n",
    "print(preds)\n",
    "# get the predicted label\n",
    "label_idx = np.argmax(logits).item()\n",
    "\n",
    "# map the label index to a label\n",
    "label_map = {0: \"negative\",1:\"neutral\", 2: \"positive\"}\n",
    "sentiment = label_map[label_idx]\n",
    "print(sentiment)\n",
    "\n",
    "\n",
    "tokens=tokenzier.tokenize(text)\n",
    "print(tokens)\n",
    "ids=tokenzier.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\ddg12\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dict_keys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ddg12\\OneDrive\\Desktop\\news_sentiment\\sentiment.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m encoding \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   sample_txt,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   max_length\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# Return PyTorch tensors\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m encoding\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m dict_keys([\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ddg12/OneDrive/Desktop/news_sentiment/sentiment.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(encoding[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dict_keys' is not defined"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=32,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "encoding.keys()\n",
    "dict_keys(['input_ids', 'attention_mask'])\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=-0.1, subjectivity=0.6)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = all_articles['title'][8]\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: transformers in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.33.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ddg12\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ddg12\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp311-cp311-win_amd64.whl (172.3 MB)\n",
      "     ---------------------------------------- 0.0/172.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/172.3 MB 6.5 MB/s eta 0:00:27\n",
      "     ---------------------------------------- 0.7/172.3 MB 7.8 MB/s eta 0:00:22\n",
      "     ---------------------------------------- 1.2/172.3 MB 8.5 MB/s eta 0:00:21\n",
      "     ---------------------------------------- 1.8/172.3 MB 9.5 MB/s eta 0:00:19\n",
      "      -------------------------------------- 2.4/172.3 MB 10.1 MB/s eta 0:00:17\n",
      "      -------------------------------------- 3.0/172.3 MB 10.8 MB/s eta 0:00:16\n",
      "      -------------------------------------- 3.7/172.3 MB 10.8 MB/s eta 0:00:16\n",
      "      -------------------------------------- 4.3/172.3 MB 11.4 MB/s eta 0:00:15\n",
      "     - ------------------------------------- 4.9/172.3 MB 11.2 MB/s eta 0:00:15\n",
      "     - ------------------------------------- 5.8/172.3 MB 11.9 MB/s eta 0:00:14\n",
      "     - ------------------------------------- 6.7/172.3 MB 12.2 MB/s eta 0:00:14\n",
      "     - ------------------------------------- 7.4/172.3 MB 12.5 MB/s eta 0:00:14\n",
      "     - ------------------------------------- 8.2/172.3 MB 12.7 MB/s eta 0:00:13\n",
      "     -- ------------------------------------ 9.1/172.3 MB 12.9 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 10.0/172.3 MB 13.1 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 11.0/172.3 MB 14.6 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 11.9/172.3 MB 15.2 MB/s eta 0:00:11\n",
      "     -- ----------------------------------- 12.8/172.3 MB 15.6 MB/s eta 0:00:11\n",
      "     -- ----------------------------------- 13.5/172.3 MB 15.6 MB/s eta 0:00:11\n",
      "     --- ---------------------------------- 14.1/172.3 MB 15.2 MB/s eta 0:00:11\n",
      "     --- ---------------------------------- 14.9/172.3 MB 16.0 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 16.3/172.3 MB 16.8 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 17.1/172.3 MB 16.8 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 18.1/172.3 MB 17.3 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 18.9/172.3 MB 17.7 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 19.9/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 20.6/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 21.5/172.3 MB 17.3 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 22.3/172.3 MB 16.8 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 23.2/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 24.0/172.3 MB 17.7 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 24.8/172.3 MB 17.7 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 25.7/172.3 MB 17.7 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 26.6/172.3 MB 17.7 MB/s eta 0:00:09\n",
      "     ------ ------------------------------- 27.5/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ------ ------------------------------- 28.4/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ------ ------------------------------- 29.2/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ------ ------------------------------- 30.1/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ------ ------------------------------- 30.9/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ------- ------------------------------ 31.8/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ------- ------------------------------ 32.6/172.3 MB 17.7 MB/s eta 0:00:08\n",
      "     ------- ------------------------------ 33.4/172.3 MB 16.8 MB/s eta 0:00:09\n",
      "     ------- ------------------------------ 34.0/172.3 MB 17.2 MB/s eta 0:00:09\n",
      "     ------- ------------------------------ 35.1/172.3 MB 17.3 MB/s eta 0:00:08\n",
      "     ------- ------------------------------ 36.0/172.3 MB 17.7 MB/s eta 0:00:08\n",
      "     -------- ----------------------------- 36.9/172.3 MB 17.2 MB/s eta 0:00:08\n",
      "     -------- ----------------------------- 37.9/172.3 MB 17.2 MB/s eta 0:00:08\n",
      "     -------- ----------------------------- 38.6/172.3 MB 17.3 MB/s eta 0:00:08\n",
      "     -------- ----------------------------- 39.5/172.3 MB 17.2 MB/s eta 0:00:08\n",
      "     -------- ----------------------------- 40.1/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 41.0/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 41.8/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 42.7/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 43.6/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 44.4/172.3 MB 17.2 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 45.1/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     ---------- --------------------------- 46.1/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     ---------- --------------------------- 47.0/172.3 MB 16.4 MB/s eta 0:00:08\n",
      "     ---------- --------------------------- 47.8/172.3 MB 16.4 MB/s eta 0:00:08\n",
      "     ---------- --------------------------- 48.5/172.3 MB 16.4 MB/s eta 0:00:08\n",
      "     ---------- --------------------------- 49.3/172.3 MB 16.4 MB/s eta 0:00:08\n",
      "     ----------- -------------------------- 50.2/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     ----------- -------------------------- 50.9/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     ----------- -------------------------- 52.0/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     ----------- -------------------------- 52.7/172.3 MB 17.2 MB/s eta 0:00:07\n",
      "     ----------- -------------------------- 53.6/172.3 MB 16.8 MB/s eta 0:00:08\n",
      "     ------------ ------------------------- 54.5/172.3 MB 17.2 MB/s eta 0:00:07\n",
      "     ------------ ------------------------- 55.4/172.3 MB 17.3 MB/s eta 0:00:07\n",
      "     ------------ ------------------------- 56.3/172.3 MB 16.8 MB/s eta 0:00:07\n",
      "     ------------ ------------------------- 57.2/172.3 MB 17.2 MB/s eta 0:00:07\n",
      "     ------------ ------------------------- 57.9/172.3 MB 16.8 MB/s eta 0:00:07\n",
      "     ------------ ------------------------- 58.9/172.3 MB 17.7 MB/s eta 0:00:07\n",
      "     ------------- ------------------------ 59.8/172.3 MB 17.2 MB/s eta 0:00:07\n",
      "     ------------- ------------------------ 60.5/172.3 MB 17.7 MB/s eta 0:00:07\n",
      "     ------------- ------------------------ 61.4/172.3 MB 18.2 MB/s eta 0:00:07\n",
      "     ------------- ------------------------ 62.2/172.3 MB 17.2 MB/s eta 0:00:07\n",
      "     ------------- ------------------------ 63.2/172.3 MB 17.7 MB/s eta 0:00:07\n",
      "     -------------- ----------------------- 63.9/172.3 MB 17.3 MB/s eta 0:00:07\n",
      "     -------------- ----------------------- 64.8/172.3 MB 17.7 MB/s eta 0:00:07\n",
      "     -------------- ----------------------- 65.5/172.3 MB 17.2 MB/s eta 0:00:07\n",
      "     -------------- ----------------------- 66.3/172.3 MB 16.8 MB/s eta 0:00:07\n",
      "     -------------- ----------------------- 66.9/172.3 MB 16.8 MB/s eta 0:00:07\n",
      "     -------------- ----------------------- 67.8/172.3 MB 16.4 MB/s eta 0:00:07\n",
      "     --------------- ---------------------- 68.4/172.3 MB 16.4 MB/s eta 0:00:07\n",
      "     --------------- ---------------------- 69.3/172.3 MB 16.4 MB/s eta 0:00:07\n",
      "     --------------- ---------------------- 70.1/172.3 MB 16.0 MB/s eta 0:00:07\n",
      "     --------------- ---------------------- 71.0/172.3 MB 16.4 MB/s eta 0:00:07\n",
      "     --------------- ---------------------- 71.8/172.3 MB 16.4 MB/s eta 0:00:07\n",
      "     --------------- ---------------------- 72.5/172.3 MB 16.0 MB/s eta 0:00:07\n",
      "     ---------------- --------------------- 73.5/172.3 MB 16.0 MB/s eta 0:00:07\n",
      "     ---------------- --------------------- 74.3/172.3 MB 16.4 MB/s eta 0:00:06\n",
      "     ---------------- --------------------- 75.1/172.3 MB 16.4 MB/s eta 0:00:06\n",
      "     ---------------- --------------------- 76.0/172.3 MB 16.8 MB/s eta 0:00:06\n",
      "     ---------------- --------------------- 76.8/172.3 MB 16.0 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 77.7/172.3 MB 16.8 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 78.6/172.3 MB 16.8 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 79.5/172.3 MB 16.8 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 80.3/172.3 MB 16.8 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 80.6/172.3 MB 16.0 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 81.5/172.3 MB 16.4 MB/s eta 0:00:06\n",
      "     ------------------ ------------------- 82.2/172.3 MB 16.0 MB/s eta 0:00:06\n",
      "     ------------------ ------------------- 83.1/172.3 MB 16.0 MB/s eta 0:00:06\n",
      "     ------------------ ------------------- 83.8/172.3 MB 16.0 MB/s eta 0:00:06\n",
      "     ------------------ ------------------- 84.5/172.3 MB 15.6 MB/s eta 0:00:06\n",
      "     ------------------ ------------------- 85.5/172.3 MB 15.6 MB/s eta 0:00:06\n",
      "     ------------------- ------------------ 86.2/172.3 MB 15.6 MB/s eta 0:00:06\n",
      "     ------------------- ------------------ 87.1/172.3 MB 16.0 MB/s eta 0:00:06\n",
      "     ------------------- ------------------ 87.9/172.3 MB 15.6 MB/s eta 0:00:06\n",
      "     ------------------- ------------------ 88.8/172.3 MB 15.2 MB/s eta 0:00:06\n",
      "     ------------------- ------------------ 89.5/172.3 MB 15.6 MB/s eta 0:00:06\n",
      "     ------------------- ------------------ 90.4/172.3 MB 15.6 MB/s eta 0:00:06\n",
      "     -------------------- ----------------- 91.0/172.3 MB 16.4 MB/s eta 0:00:05\n",
      "     -------------------- ----------------- 91.7/172.3 MB 15.6 MB/s eta 0:00:06\n",
      "     -------------------- ----------------- 92.5/172.3 MB 16.0 MB/s eta 0:00:05\n",
      "     -------------------- ----------------- 93.3/172.3 MB 16.0 MB/s eta 0:00:05\n",
      "     -------------------- ----------------- 93.9/172.3 MB 15.6 MB/s eta 0:00:06\n",
      "     -------------------- ----------------- 94.9/172.3 MB 16.0 MB/s eta 0:00:05\n",
      "     --------------------- ---------------- 95.8/172.3 MB 16.4 MB/s eta 0:00:05\n",
      "     --------------------- ---------------- 96.7/172.3 MB 16.4 MB/s eta 0:00:05\n",
      "     --------------------- ---------------- 97.7/172.3 MB 16.4 MB/s eta 0:00:05\n",
      "     --------------------- ---------------- 98.4/172.3 MB 16.0 MB/s eta 0:00:05\n",
      "     --------------------- ---------------- 99.3/172.3 MB 16.4 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 100.0/172.3 MB 16.4 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 100.8/172.3 MB 16.4 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 101.7/172.3 MB 16.8 MB/s eta 0:00:05\n",
      "     ---------------------- -------------- 102.6/172.3 MB 16.8 MB/s eta 0:00:05\n",
      "     ---------------------- -------------- 103.4/172.3 MB 16.8 MB/s eta 0:00:05\n",
      "     ---------------------- -------------- 104.4/172.3 MB 17.7 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 105.2/172.3 MB 17.3 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 106.0/172.3 MB 17.2 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 107.0/172.3 MB 17.7 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 107.9/172.3 MB 17.2 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 108.7/172.3 MB 17.7 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 109.4/172.3 MB 17.3 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 110.3/172.3 MB 17.2 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 111.1/172.3 MB 17.7 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 111.5/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 112.3/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 113.2/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 114.1/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 115.0/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 115.8/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 116.8/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 117.5/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 118.6/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 119.5/172.3 MB 16.8 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 120.5/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 121.3/172.3 MB 16.4 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 122.3/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 123.0/172.3 MB 17.3 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 123.9/172.3 MB 17.3 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 124.8/172.3 MB 17.7 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 125.6/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 126.4/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 127.1/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 128.1/172.3 MB 17.7 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 128.8/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 129.7/172.3 MB 16.8 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 130.6/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 131.5/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 132.3/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 133.1/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 134.0/172.3 MB 17.3 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 134.9/172.3 MB 16.8 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 135.8/172.3 MB 17.2 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 136.7/172.3 MB 17.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 137.4/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 138.4/172.3 MB 17.2 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 139.3/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 140.1/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 141.1/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 141.8/172.3 MB 17.2 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 142.7/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 143.6/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 144.4/172.3 MB 17.2 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 145.3/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 146.1/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 147.0/172.3 MB 17.2 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 147.9/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 148.8/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 149.7/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 150.5/172.3 MB 17.2 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 151.4/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 152.3/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 153.2/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 154.0/172.3 MB 17.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 154.9/172.3 MB 17.2 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 155.8/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 156.6/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 157.5/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 158.2/172.3 MB 18.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 159.3/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 160.1/172.3 MB 18.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 161.0/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 161.8/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 162.8/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 163.7/172.3 MB 17.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 164.5/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 165.3/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 166.1/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 167.0/172.3 MB 17.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  167.9/172.3 MB 17.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  168.7/172.3 MB 17.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  169.5/172.3 MB 17.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  170.4/172.3 MB 17.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  171.2/172.3 MB 17.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.2/172.3 MB 17.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 16.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 172.3/172.3 MB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.7.1)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.1 sympy-1.12 torch-2.0.1\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Collecting nltk>=3.1 (from textblob)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click (from nltk>=3.1->textblob)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>=3.1->textblob)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ddg12\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: joblib, click, nltk, textblob\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 textblob-0.17.1\n",
      "Requirement already satisfied: pandas in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ddg12\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ddg12\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.25.2)\n",
      "Collecting newsapi-python\n",
      "  Using cached newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from newsapi-python) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0->newsapi-python) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0->newsapi-python) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0->newsapi-python) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ddg12\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0->newsapi-python) (2023.7.22)\n",
      "Installing collected packages: newsapi-python\n",
      "Successfully installed newsapi-python-0.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install textblob\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
